
"""
Typically:
1. Training for longer number of epochs tends to increase performance (but early stopping also works in some cases).
2. Increased size of hidden layers aka width increases capacity leading to double descent.
3. Increasing number of hidden layers aka depth typically makes the network more expressive (but networks that're very deep overfit).
""";